#!/usr/bin/env python3
"""
å¤æ‚å¤šæ­¥éª¤ä»»åŠ¡çš„è®°å¿†ç®¡ç†æµ‹è¯•
æµ‹è¯•è®°å¿†ç³»ç»Ÿåœ¨å¤æ‚å·¥ä½œæµä¸­çš„è¡¨ç°
"""

import sys
import asyncio
import time
import json
from pathlib import Path
from typing import List, Dict, Any
import random

# æ·»åŠ è®°å¿†ç³»ç»Ÿåˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "claude-memory-system"))

from claude_memory import MemoryManager

class ComplexScenarioTester:
    """å¤æ‚åœºæ™¯æµ‹è¯•å™¨"""
    
    def __init__(self, project_path: Path):
        self.memory = MemoryManager(project_path=project_path)
        self.scenario_results = []
        
    async def ensure_memory_sync(self):
        """ç¡®ä¿è®°å¿†åŒæ­¥åˆ°å‘é‡å­˜å‚¨"""
        # è‹é†’ç³»ç»Ÿ
        snapshot = self.memory.awaken("å¤æ‚åœºæ™¯æµ‹è¯•")
        
        # æ‰‹åŠ¨åŒæ­¥æ–‡ä»¶å­˜å‚¨åˆ°å‘é‡å­˜å‚¨
        all_memories = self.memory._file_store.load_all_memories()
        for mem in all_memories:
            if mem.id not in self.memory._vector_store._memory_cache:
                self.memory._vector_store.store_memory(mem)
        
        print(f"âœ… è®°å¿†ç³»ç»Ÿå·²åŒæ­¥ï¼Œæ€»è®°å¿†æ•°: {len(all_memories)}")
        return len(all_memories)
    
    async def test_multi_step_project_development(self):
        """æµ‹è¯•å¤šæ­¥éª¤é¡¹ç›®å¼€å‘åœºæ™¯"""
        print("\nğŸš€ æµ‹è¯•åœºæ™¯1: å¤šæ­¥éª¤Webåº”ç”¨å¼€å‘")
        print("-" * 50)
        
        # æ¨¡æ‹Ÿä¸€ä¸ªå®Œæ•´çš„Webåº”ç”¨å¼€å‘æµç¨‹
        development_steps = [
            {
                "step": "éœ€æ±‚åˆ†æ",
                "action": "åˆ†æç”¨æˆ·éœ€æ±‚ï¼Œè®¾è®¡ç³»ç»Ÿæ¶æ„",
                "search_query": "webåº”ç”¨æ¶æ„è®¾è®¡",
                "memory_content": """
Webåº”ç”¨éœ€æ±‚åˆ†æç»“æœï¼š
- ç”¨æˆ·ç®¡ç†ç³»ç»Ÿï¼šæ³¨å†Œã€ç™»å½•ã€æƒé™æ§åˆ¶
- å†…å®¹ç®¡ç†ï¼šæ–‡ç« å‘å¸ƒã€ç¼–è¾‘ã€åˆ é™¤
- è¯„è®ºç³»ç»Ÿï¼šç”¨æˆ·è¯„è®ºã€ç‚¹èµã€å›å¤
- æœç´¢åŠŸèƒ½ï¼šå…¨æ–‡æœç´¢ã€æ ‡ç­¾ç­›é€‰
- æ•°æ®ç»Ÿè®¡ï¼šç”¨æˆ·è¡Œä¸ºåˆ†æã€å†…å®¹çƒ­åº¦

æŠ€æœ¯é€‰å‹ï¼š
- å‰ç«¯ï¼šReact + TypeScript + Tailwind CSS
- åç«¯ï¼šNode.js + Express + PostgreSQL
- éƒ¨ç½²ï¼šDocker + Kubernetes + AWS
                """,
                "tags": ["éœ€æ±‚åˆ†æ", "æ¶æ„è®¾è®¡", "webåº”ç”¨"]
            },
            {
                "step": "æ•°æ®åº“è®¾è®¡",
                "action": "è®¾è®¡æ•°æ®åº“è¡¨ç»“æ„",
                "search_query": "æ•°æ®åº“è®¾è®¡æœ€ä½³å®è·µ",
                "memory_content": """
Webåº”ç”¨æ•°æ®åº“è®¾è®¡ï¼š

ç”¨æˆ·è¡¨ (users):
- id (PK), username, email, password_hash
- created_at, updated_at, last_login
- role (admin/editor/user), status (active/inactive)

æ–‡ç« è¡¨ (articles):
- id (PK), title, content, author_id (FK)
- published_at, updated_at, status (draft/published)
- category_id (FK), view_count, like_count

è¯„è®ºè¡¨ (comments):
- id (PK), article_id (FK), user_id (FK)
- content, parent_id (FK), created_at
- status (approved/pending/deleted)

ç´¢å¼•ä¼˜åŒ–ï¼š
- articles: (author_id, published_at), (category_id)
- comments: (article_id, created_at), (user_id)
                """,
                "tags": ["æ•°æ®åº“è®¾è®¡", "PostgreSQL", "ç´¢å¼•ä¼˜åŒ–"]
            },
            {
                "step": "APIå¼€å‘",
                "action": "å¼€å‘REST APIæ¥å£",
                "search_query": "Node.js Express APIå¼€å‘",
                "memory_content": """
Webåº”ç”¨APIè®¾è®¡ï¼š

ç”¨æˆ·è®¤è¯ API:
POST /api/auth/register - ç”¨æˆ·æ³¨å†Œ
POST /api/auth/login - ç”¨æˆ·ç™»å½•
POST /api/auth/logout - ç”¨æˆ·ç™»å‡º
GET /api/auth/profile - è·å–ç”¨æˆ·ä¿¡æ¯

æ–‡ç« ç®¡ç† API:
GET /api/articles - è·å–æ–‡ç« åˆ—è¡¨ (åˆ†é¡µ+ç­›é€‰)
GET /api/articles/:id - è·å–æ–‡ç« è¯¦æƒ…
POST /api/articles - åˆ›å»ºæ–‡ç«  (éœ€è®¤è¯)
PUT /api/articles/:id - æ›´æ–°æ–‡ç«  (éœ€æƒé™)
DELETE /api/articles/:id - åˆ é™¤æ–‡ç«  (éœ€æƒé™)

è¯„è®ºç³»ç»Ÿ API:
GET /api/articles/:id/comments - è·å–è¯„è®ºåˆ—è¡¨
POST /api/articles/:id/comments - æ·»åŠ è¯„è®º (éœ€è®¤è¯)
PUT /api/comments/:id - æ›´æ–°è¯„è®º (éœ€æƒé™)
DELETE /api/comments/:id - åˆ é™¤è¯„è®º (éœ€æƒé™)

ä¸­é—´ä»¶å®ç°ï¼š
- èº«ä»½è®¤è¯ï¼šJWT tokenéªŒè¯
- æƒé™æ§åˆ¶ï¼šåŸºäºè§’è‰²çš„è®¿é—®æ§åˆ¶
- è¾“å…¥éªŒè¯ï¼šå‚æ•°æ ¡éªŒå’ŒXSSé˜²æŠ¤
- é”™è¯¯å¤„ç†ï¼šç»Ÿä¸€é”™è¯¯å“åº”æ ¼å¼
                """,
                "tags": ["APIå¼€å‘", "Node.js", "Express", "JWT", "æƒé™æ§åˆ¶"]
            },
            {
                "step": "å‰ç«¯å¼€å‘",
                "action": "å¼€å‘Reactå‰ç«¯ç•Œé¢",
                "search_query": "React TypeScriptæœ€ä½³å®è·µ",
                "memory_content": """
Reactå‰ç«¯å¼€å‘å®ç°ï¼š

é¡¹ç›®ç»“æ„ï¼š
src/
  components/ - å¯å¤ç”¨ç»„ä»¶
    UI/ - åŸºç¡€UIç»„ä»¶ (Button, Input, Modal)
    Layout/ - å¸ƒå±€ç»„ä»¶ (Header, Sidebar, Footer)
  pages/ - é¡µé¢ç»„ä»¶
    Auth/ - ç™»å½•æ³¨å†Œé¡µé¢
    Articles/ - æ–‡ç« ç›¸å…³é¡µé¢
    Profile/ - ç”¨æˆ·ä¸­å¿ƒé¡µé¢
  hooks/ - è‡ªå®šä¹‰hooks
    useAuth.ts - è®¤è¯çŠ¶æ€ç®¡ç†
    useAPI.ts - APIè°ƒç”¨å°è£…
  store/ - çŠ¶æ€ç®¡ç† (Redux Toolkit)
  utils/ - å·¥å…·å‡½æ•°

å…³é”®å®ç°ï¼š
- è·¯ç”±ç®¡ç†ï¼šReact Router v6
- çŠ¶æ€ç®¡ç†ï¼šRedux Toolkit + RTK Query
- æ ·å¼æ–¹æ¡ˆï¼šTailwind CSS + CSS Modules
- è¡¨å•å¤„ç†ï¼šReact Hook Form + YupéªŒè¯
- æƒé™æ§åˆ¶ï¼šè·¯ç”±å®ˆå« + ç»„ä»¶çº§æƒé™
                """,
                "tags": ["React", "TypeScript", "å‰ç«¯å¼€å‘", "çŠ¶æ€ç®¡ç†"]
            },
            {
                "step": "æµ‹è¯•å¼€å‘",
                "action": "ç¼–å†™å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•",
                "search_query": "Node.js Reactæµ‹è¯•æœ€ä½³å®è·µ",
                "memory_content": """
Webåº”ç”¨æµ‹è¯•ç­–ç•¥ï¼š

åç«¯æµ‹è¯• (Jest + Supertest):
- å•å…ƒæµ‹è¯•ï¼šæ¨¡å‹éªŒè¯ã€å·¥å…·å‡½æ•°
- APIæµ‹è¯•ï¼šæ¯ä¸ªç«¯ç‚¹çš„CRUDæ“ä½œ
- é›†æˆæµ‹è¯•ï¼šæ•°æ®åº“äº¤äº’ã€è®¤è¯æµç¨‹
- ä¸­é—´ä»¶æµ‹è¯•ï¼šæƒé™éªŒè¯ã€é”™è¯¯å¤„ç†

å‰ç«¯æµ‹è¯• (Jest + React Testing Library):
- ç»„ä»¶æµ‹è¯•ï¼šæ¸²æŸ“ã€ç”¨æˆ·äº¤äº’ã€çŠ¶æ€å˜åŒ–
- é¡µé¢æµ‹è¯•ï¼šè·¯ç”±å¯¼èˆªã€æ•°æ®è·å–
- Hookæµ‹è¯•ï¼šè‡ªå®šä¹‰hooksçš„é€»è¾‘
- ç«¯åˆ°ç«¯æµ‹è¯•ï¼šCypresså®Œæ•´æµç¨‹æµ‹è¯•

æµ‹è¯•è¦†ç›–ç‡ç›®æ ‡ï¼š
- åç«¯APIï¼š90%ä»¥ä¸Šä»£ç è¦†ç›–ç‡
- å‰ç«¯ç»„ä»¶ï¼š85%ä»¥ä¸Šè¦†ç›–ç‡
- å…³é”®ä¸šåŠ¡æµç¨‹ï¼š100%è¦†ç›–

CI/CDé›†æˆï¼š
- GitHub Actionsè‡ªåŠ¨åŒ–æµ‹è¯•
- ä»£ç è´¨é‡æ£€æŸ¥ï¼šESLint + Prettier
- å®‰å…¨æ‰«æï¼šnpm audit + Snyk
                """,
                "tags": ["æµ‹è¯•å¼€å‘", "Jest", "React Testing Library", "CI/CD"]
            },
            {
                "step": "éƒ¨ç½²ä¸Šçº¿",
                "action": "éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ",
                "search_query": "Docker Kuberneteséƒ¨ç½²",
                "memory_content": """
Webåº”ç”¨éƒ¨ç½²æ–¹æ¡ˆï¼š

å®¹å™¨åŒ–é…ç½®ï¼š
- Frontend Dockerfileï¼šå¤šé˜¶æ®µæ„å»ºï¼Œnginxé™æ€æ‰˜ç®¡
- Backend Dockerfileï¼šNode.jsè¿è¡Œæ—¶ä¼˜åŒ–
- PostgreSQLï¼šå®˜æ–¹é•œåƒ+æŒä¹…åŒ–å­˜å‚¨
- Redisï¼šç¼“å­˜å’Œä¼šè¯å­˜å‚¨

Kuberneteséƒ¨ç½²ï¼š
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-backend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webapp-backend
  template:
    spec:
      containers:
      - name: backend
        image: webapp-backend:latest
        ports:
        - containerPort: 3000
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: webapp-secrets
              key: database-url

æœåŠ¡é…ç½®ï¼š
- Ingressï¼šåŸŸåè·¯ç”±å’ŒSSLç»ˆæ­¢
- Serviceï¼šè´Ÿè½½å‡è¡¡å’ŒæœåŠ¡å‘ç°
- ConfigMapï¼šåº”ç”¨é…ç½®ç®¡ç†
- Secretï¼šæ•æ„Ÿä¿¡æ¯åŠ å¯†å­˜å‚¨

ç›‘æ§å‘Šè­¦ï¼š
- Prometheusï¼šæŒ‡æ ‡æ”¶é›†
- Grafanaï¼šå¯è§†åŒ–ç›‘æ§é¢æ¿
- AlertManagerï¼šå‘Šè­¦è§„åˆ™å’Œé€šçŸ¥
                """,
                "tags": ["éƒ¨ç½²", "Docker", "Kubernetes", "ç›‘æ§"]
            }
        ]
        
        step_memories = []
        context_evolution = []
        
        for i, step_info in enumerate(development_steps, 1):
            print(f"\næ­¥éª¤ {i}: {step_info['step']}")
            
            # 1. åŸºäºå‰é¢çš„è®°å¿†æœç´¢ç›¸å…³ç»éªŒ
            if step_memories:
                print("  ğŸ“š æœç´¢ç›¸å…³è®°å¿†...")
                
                # æ„å»ºåŸºäºä¸Šä¸‹æ–‡çš„æœç´¢æŸ¥è¯¢
                context_tags = []
                for mem in step_memories[-2:]:  # ä½¿ç”¨æœ€è¿‘2ä¸ªè®°å¿†ä½œä¸ºä¸Šä¸‹æ–‡
                    context_tags.extend(mem.memory.tags)
                
                enhanced_query = f"{step_info['search_query']} {' '.join(set(context_tags[-5:]))}"
                search_results = self.memory.recall(enhanced_query, max_results=3, min_relevance=0.1)
                
                print(f"     æŸ¥è¯¢: {step_info['search_query']}")
                print(f"     ä¸Šä¸‹æ–‡å¢å¼º: {enhanced_query}")
                print(f"     æ‰¾åˆ° {len(search_results)} ä¸ªç›¸å…³è®°å¿†")
                
                # åˆ†æä¸Šä¸‹æ–‡è¿è´¯æ€§
                if search_results:
                    for result in search_results:
                        context_overlap = len(set(context_tags) & set(result.memory.tags))
                        print(f"       - {result.memory.title} (ç›¸å…³æ€§: {result.relevance_score:.3f}, ä¸Šä¸‹æ–‡é‡å : {context_overlap})")
            
            # 2. å­˜å‚¨å½“å‰æ­¥éª¤çš„è®°å¿†
            print("  ğŸ’¾ å­˜å‚¨æ­¥éª¤è®°å¿†...")
            memory_id = self.memory.remember(
                content=step_info["memory_content"],
                memory_type="episodic",
                title=f"Webåº”ç”¨å¼€å‘ - {step_info['step']}",
                tags=step_info["tags"] + ["webåº”ç”¨å¼€å‘", f"ç¬¬{i}æ­¥"],
                importance=8.0 + (i * 0.2),  # é€’å¢é‡è¦æ€§
                scope="project"
            )
            
            # è·å–åˆšå­˜å‚¨çš„è®°å¿†
            stored_memory = self.memory._file_store.load_memory(memory_id)
            if stored_memory:
                # ç¡®ä¿åœ¨å‘é‡å­˜å‚¨ä¸­
                if memory_id not in self.memory._vector_store._memory_cache:
                    self.memory._vector_store.store_memory(stored_memory)
                
                step_memories.append(type('MemoryResult', (), {
                    'memory': stored_memory,
                    'relevance_score': 1.0
                })())
            
            # 3. åˆ†æå½“å‰ä¸Šä¸‹æ–‡çŠ¶æ€
            current_context = {
                "step": i,
                "action": step_info["action"],
                "accumulated_tags": list(set(sum([mem.memory.tags for mem in step_memories], []))),
                "memory_count": len(step_memories)
            }
            context_evolution.append(current_context)
            
            print(f"     å­˜å‚¨è®°å¿†ID: {memory_id[:8]}...")
            print(f"     ç´¯ç§¯æ ‡ç­¾æ•°: {len(current_context['accumulated_tags'])}")
            
            # 4. åŸºäºå½“å‰ä¸Šä¸‹æ–‡è·å–å»ºè®®
            if i > 2:  # ä»ç¬¬3æ­¥å¼€å§‹è·å–å»ºè®®
                suggestions = self.memory.suggest(f"æ­£åœ¨å¼€å‘Webåº”ç”¨ï¼Œå·²å®Œæˆ{step_info['step']}ï¼Œä¸‹ä¸€æ­¥è®¡åˆ’")
                if suggestions:
                    print(f"  ğŸ’¡ æ™ºèƒ½å»ºè®®:")
                    for j, suggestion in enumerate(suggestions[:2], 1):
                        print(f"       {j}. {suggestion.action}")
        
        # åˆ†ææ•´ä¸ªå¼€å‘æµç¨‹çš„è®°å¿†è¿è´¯æ€§
        print(f"\nğŸ“Š å¼€å‘æµç¨‹è®°å¿†åˆ†æ:")
        
        # è®¡ç®—æ­¥éª¤é—´çš„è¯­ä¹‰è¿è´¯æ€§
        coherence_scores = []
        for i in range(len(step_memories) - 1):
            tags1 = set(step_memories[i].memory.tags)
            tags2 = set(step_memories[i+1].memory.tags)
            
            # è®¡ç®—æ ‡ç­¾é‡å åº¦
            overlap = len(tags1 & tags2)
            union = len(tags1 | tags2)
            coherence = overlap / union if union > 0 else 0
            
            coherence_scores.append(coherence)
            print(f"  æ­¥éª¤{i+1}â†’{i+2} è¿è´¯æ€§: {coherence:.3f}")
        
        avg_coherence = sum(coherence_scores) / len(coherence_scores) if coherence_scores else 0
        
        # æµ‹è¯•è·¨æ­¥éª¤çš„è®°å¿†æ£€ç´¢
        print(f"\nğŸ” è·¨æ­¥éª¤è®°å¿†æ£€ç´¢æµ‹è¯•:")
        cross_step_queries = [
            "æ•°æ®åº“APIé›†æˆ",
            "å‰ç«¯åç«¯å¯¹æ¥",
            "æµ‹è¯•éƒ¨ç½²æµç¨‹",
            "å®Œæ•´å¼€å‘æµç¨‹"
        ]
        
        retrieval_accuracy = []
        for query in cross_step_queries:
            results = self.memory.recall(query, max_results=5, min_relevance=0.1)
            
            # æ£€æŸ¥æ˜¯å¦èƒ½æ‰¾åˆ°ç›¸å…³çš„å¼€å‘æ­¥éª¤
            found_steps = set()
            for result in results:
                if "webåº”ç”¨å¼€å‘" in result.memory.tags:
                    for tag in result.memory.tags:
                        if tag.startswith("ç¬¬") and tag.endswith("æ­¥"):
                            found_steps.add(tag)
            
            accuracy = len(found_steps) / len(development_steps)
            retrieval_accuracy.append(accuracy)
            
            print(f"  '{query}': æ‰¾åˆ° {len(results)} ä¸ªç»“æœ, è¦†ç›– {len(found_steps)} ä¸ªå¼€å‘æ­¥éª¤")
        
        avg_retrieval_accuracy = sum(retrieval_accuracy) / len(retrieval_accuracy)
        
        return {
            "scenario": "å¤šæ­¥éª¤Webåº”ç”¨å¼€å‘",
            "total_steps": len(development_steps),
            "memories_created": len(step_memories),
            "avg_coherence": avg_coherence,
            "avg_retrieval_accuracy": avg_retrieval_accuracy,
            "context_evolution": context_evolution
        }
    
    async def test_knowledge_domain_integration(self):
        """æµ‹è¯•çŸ¥è¯†é¢†åŸŸæ•´åˆåœºæ™¯"""
        print("\nğŸ§  æµ‹è¯•åœºæ™¯2: çŸ¥è¯†é¢†åŸŸæ•´åˆåº”ç”¨")
        print("-" * 50)
        
        # æ¨¡æ‹Ÿéœ€è¦æ•´åˆå¤šä¸ªæŠ€æœ¯é¢†åŸŸçš„å¤æ‚ä»»åŠ¡
        integration_tasks = [
            {
                "task": "AIé©±åŠ¨çš„æ¨èç³»ç»Ÿæ¶æ„",
                "domains": ["æœºå™¨å­¦ä¹ ", "åˆ†å¸ƒå¼ç³»ç»Ÿ", "æ•°æ®åº“"],
                "search_queries": [
                    "æœºå™¨å­¦ä¹ æ¨èç®—æ³•",
                    "åˆ†å¸ƒå¼è®¡ç®—æ¶æ„",
                    "å¤§æ•°æ®å­˜å‚¨æ–¹æ¡ˆ"
                ]
            },
            {
                "task": "å¾®æœåŠ¡æ¶æ„çš„ç›‘æ§ç³»ç»Ÿ",
                "domains": ["å®¹å™¨åŒ–", "ç›‘æ§", "åˆ†å¸ƒå¼ç³»ç»Ÿ"],
                "search_queries": [
                    "KubernetesæœåŠ¡ç›‘æ§",
                    "åˆ†å¸ƒå¼æ—¥å¿—æ”¶é›†",
                    "å¾®æœåŠ¡é“¾è·¯è¿½è¸ª"
                ]
            },
            {
                "task": "é«˜æ€§èƒ½æ•°æ®åˆ†æå¹³å°",
                "domains": ["æ•°æ®åº“", "æ€§èƒ½ä¼˜åŒ–", "æœºå™¨å­¦ä¹ "],
                "search_queries": [
                    "æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–",
                    "å¤§æ•°æ®å¤„ç†æ¡†æ¶",
                    "å®æ—¶æ•°æ®åˆ†æ"
                ]
            }
        ]
        
        integration_results = []
        
        for task_info in integration_tasks:
            print(f"\nä»»åŠ¡: {task_info['task']}")
            print(f"æ¶‰åŠé¢†åŸŸ: {', '.join(task_info['domains'])}")
            
            # å¯¹æ¯ä¸ªé¢†åŸŸè¿›è¡Œè®°å¿†æœç´¢
            domain_memories = {}
            total_relevance = 0
            found_domains = set()
            
            for domain, query in zip(task_info['domains'], task_info['search_queries']):
                print(f"\n  æœç´¢ {domain}: '{query}'")
                
                results = self.memory.recall(query, max_results=3, min_relevance=0.1)
                domain_memories[domain] = results
                
                if results:
                    best_result = results[0]
                    total_relevance += best_result.relevance_score
                    
                    # æ£€æŸ¥æ‰¾åˆ°çš„è®°å¿†æ˜¯å¦çœŸçš„å±äºè¯¥é¢†åŸŸ
                    for tag in best_result.memory.tags:
                        if domain.lower() in tag.lower() or any(d.lower() in tag.lower() for d in task_info['domains']):
                            found_domains.add(domain)
                            break
                    
                    print(f"    æœ€ä½³åŒ¹é…: {best_result.memory.title}")
                    print(f"    ç›¸å…³æ€§: {best_result.relevance_score:.3f}")
                    print(f"    æ ‡ç­¾: {best_result.memory.tags}")
                else:
                    print(f"    æœªæ‰¾åˆ°ç›¸å…³è®°å¿†")
            
            # åˆ†æé¢†åŸŸæ•´åˆåº¦
            domain_coverage = len(found_domains) / len(task_info['domains'])
            avg_relevance = total_relevance / len(task_info['domains']) if task_info['domains'] else 0
            
            # å°è¯•è·¨é¢†åŸŸæ•´åˆæœç´¢
            print(f"\n  è·¨é¢†åŸŸæ•´åˆæœç´¢:")
            integrated_query = " ".join(task_info['domains'])
            integrated_results = self.memory.recall(integrated_query, max_results=5, min_relevance=0.05)
            
            cross_domain_score = 0
            for result in integrated_results:
                # è®¡ç®—è¯¥è®°å¿†æ¶µç›–å¤šå°‘ä¸ªç›®æ ‡é¢†åŸŸ
                covered_domains = 0
                for domain in task_info['domains']:
                    if any(domain.lower() in tag.lower() for tag in result.memory.tags):
                        covered_domains += 1
                
                cross_domain_score += covered_domains / len(task_info['domains'])
            
            cross_domain_avg = cross_domain_score / len(integrated_results) if integrated_results else 0
            
            result = {
                "task": task_info['task'],
                "domain_coverage": domain_coverage,
                "avg_relevance": avg_relevance,
                "cross_domain_score": cross_domain_avg,
                "memories_found": sum(len(memories) for memories in domain_memories.values())
            }
            
            integration_results.append(result)
            
            print(f"  ğŸ“Š ç»“æœ: é¢†åŸŸè¦†ç›–{domain_coverage:.1%}, å¹³å‡ç›¸å…³æ€§{avg_relevance:.3f}, è·¨åŸŸæ•´åˆ{cross_domain_avg:.3f}")
        
        return integration_results
    
    async def test_long_term_memory_persistence(self):
        """æµ‹è¯•é•¿æœŸè®°å¿†æŒä¹…åŒ–åœºæ™¯"""
        print("\nâ° æµ‹è¯•åœºæ™¯3: é•¿æœŸè®°å¿†æŒä¹…åŒ–")
        print("-" * 50)
        
        # æ¨¡æ‹Ÿé•¿æœŸä½¿ç”¨è¿‡ç¨‹ä¸­çš„è®°å¿†ç§¯ç´¯å’Œæ£€ç´¢
        print("  ğŸ“š åˆ†æç°æœ‰è®°å¿†åˆ†å¸ƒ...")
        
        # åˆ†æè®°å¿†çš„æ—¶é—´åˆ†å¸ƒå’Œç±»å‹åˆ†å¸ƒ
        all_memories = self.memory._file_store.load_all_memories()
        
        memory_stats = {
            "total": len(all_memories),
            "by_type": {},
            "by_scope": {},
            "by_importance": {"high": 0, "medium": 0, "low": 0},
            "avg_importance": 0
        }
        
        importance_sum = 0
        for memory in all_memories:
            # ç±»å‹åˆ†å¸ƒ
            mem_type = memory.memory_type.value
            memory_stats["by_type"][mem_type] = memory_stats["by_type"].get(mem_type, 0) + 1
            
            # èŒƒå›´åˆ†å¸ƒ
            scope = memory.scope.value
            memory_stats["by_scope"][scope] = memory_stats["by_scope"].get(scope, 0) + 1
            
            # é‡è¦æ€§åˆ†å¸ƒ
            importance = memory.importance
            importance_sum += importance
            
            if importance >= 8.0:
                memory_stats["by_importance"]["high"] += 1
            elif importance >= 5.0:
                memory_stats["by_importance"]["medium"] += 1
            else:
                memory_stats["by_importance"]["low"] += 1
        
        memory_stats["avg_importance"] = importance_sum / len(all_memories) if all_memories else 0
        
        print(f"    æ€»è®°å¿†æ•°: {memory_stats['total']}")
        print(f"    ç±»å‹åˆ†å¸ƒ: {memory_stats['by_type']}")
        print(f"    èŒƒå›´åˆ†å¸ƒ: {memory_stats['by_scope']}")
        print(f"    é‡è¦æ€§åˆ†å¸ƒ: {memory_stats['by_importance']}")
        print(f"    å¹³å‡é‡è¦æ€§: {memory_stats['avg_importance']:.2f}")
        
        # æµ‹è¯•è®°å¿†çš„æ—¶é—´è¡°å‡æ•ˆåº”
        print("\n  ğŸ• æµ‹è¯•è®°å¿†æ£€ç´¢çš„æ—¶é—´æ•ˆåº”...")
        
        # æ¨¡æ‹Ÿä¸åŒæ—¶æœŸçš„æŸ¥è¯¢
        time_based_queries = [
            "æœ€è¿‘çš„ç¼–ç¨‹ç»éªŒ",
            "Pythonæ€§èƒ½ä¼˜åŒ–",
            "ç³»ç»Ÿæ¶æ„è®¾è®¡",
            "æ•°æ®åº“ä¼˜åŒ–æŠ€å·§"
        ]
        
        retrieval_consistency = []
        for query in time_based_queries:
            # å¤šæ¬¡æœç´¢åŒæ ·çš„æŸ¥è¯¢ï¼Œæ£€æŸ¥ç»“æœä¸€è‡´æ€§
            results_sets = []
            for _ in range(3):
                results = self.memory.recall(query, max_results=5, min_relevance=0.1)
                results_sets.append([r.memory.id for r in results])
            
            # è®¡ç®—ç»“æœä¸€è‡´æ€§
            if results_sets:
                first_set = set(results_sets[0])
                consistency_scores = []
                for result_set in results_sets[1:]:
                    overlap = len(first_set & set(result_set))
                    consistency = overlap / len(first_set | set(result_set)) if (first_set | set(result_set)) else 1.0
                    consistency_scores.append(consistency)
                
                avg_consistency = sum(consistency_scores) / len(consistency_scores) if consistency_scores else 1.0
                retrieval_consistency.append(avg_consistency)
                
                print(f"    '{query}': ä¸€è‡´æ€§ {avg_consistency:.3f}")
        
        avg_retrieval_consistency = sum(retrieval_consistency) / len(retrieval_consistency) if retrieval_consistency else 0
        
        return {
            "memory_stats": memory_stats,
            "retrieval_consistency": avg_retrieval_consistency,
            "persistence_quality": memory_stats["avg_importance"] / 10.0  # å½’ä¸€åŒ–åˆ°0-1
        }
    
    async def run_comprehensive_complex_test(self):
        """è¿è¡Œç»¼åˆå¤æ‚åœºæ™¯æµ‹è¯•"""
        print("ğŸ§ª å¼€å§‹å¤æ‚å¤šæ­¥éª¤ä»»åŠ¡è®°å¿†ç®¡ç†æµ‹è¯•")
        print("=" * 60)
        
        # ç¡®ä¿è®°å¿†ç³»ç»ŸåŒæ­¥
        total_memories = await self.ensure_memory_sync()
        
        # è¿è¡Œå„ä¸ªæµ‹è¯•åœºæ™¯
        scenario1_result = await self.test_multi_step_project_development()
        scenario2_result = await self.test_knowledge_domain_integration()
        scenario3_result = await self.test_long_term_memory_persistence()
        
        # ç»¼åˆè¯„ä¼°
        print("\nğŸ“Š ç»¼åˆå¤æ‚åœºæ™¯æµ‹è¯•ç»“æœ")
        print("=" * 50)
        
        # åœºæ™¯1è¯„ä¼° - å¤šæ­¥éª¤å¼€å‘
        print(f"åœºæ™¯1 - å¤šæ­¥éª¤å¼€å‘:")
        print(f"  æ­¥éª¤è¿è´¯æ€§: {scenario1_result['avg_coherence']:.3f}")
        print(f"  è·¨æ­¥éª¤æ£€ç´¢å‡†ç¡®æ€§: {scenario1_result['avg_retrieval_accuracy']:.3f}")
        
        # åœºæ™¯2è¯„ä¼° - çŸ¥è¯†æ•´åˆ
        integration_scores = [r['domain_coverage'] for r in scenario2_result]
        relevance_scores = [r['avg_relevance'] for r in scenario2_result]
        cross_domain_scores = [r['cross_domain_score'] for r in scenario2_result]
        
        avg_integration = sum(integration_scores) / len(integration_scores) if integration_scores else 0
        avg_relevance = sum(relevance_scores) / len(relevance_scores) if relevance_scores else 0
        avg_cross_domain = sum(cross_domain_scores) / len(cross_domain_scores) if cross_domain_scores else 0
        
        print(f"åœºæ™¯2 - çŸ¥è¯†é¢†åŸŸæ•´åˆ:")
        print(f"  é¢†åŸŸè¦†ç›–ç‡: {avg_integration:.3f}")
        print(f"  æœç´¢ç›¸å…³æ€§: {avg_relevance:.3f}")
        print(f"  è·¨åŸŸæ•´åˆèƒ½åŠ›: {avg_cross_domain:.3f}")
        
        # åœºæ™¯3è¯„ä¼° - é•¿æœŸæŒä¹…åŒ–
        print(f"åœºæ™¯3 - é•¿æœŸè®°å¿†æŒä¹…åŒ–:")
        print(f"  æ£€ç´¢ä¸€è‡´æ€§: {scenario3_result['retrieval_consistency']:.3f}")
        print(f"  è®°å¿†è´¨é‡: {scenario3_result['persistence_quality']:.3f}")
        
        # æ•´ä½“è¯„åˆ†
        overall_score = (
            (scenario1_result['avg_coherence'] + scenario1_result['avg_retrieval_accuracy']) / 2 * 0.4 +
            (avg_integration + avg_relevance + avg_cross_domain) / 3 * 0.4 +
            (scenario3_result['retrieval_consistency'] + scenario3_result['persistence_quality']) / 2 * 0.2
        )
        
        print(f"\nğŸ¯ å¤æ‚åœºæ™¯æ•´ä½“è¯„åˆ†: {overall_score:.3f}/1.0")
        
        if overall_score >= 0.7:
            print("âœ… è®°å¿†ç³»ç»Ÿåœ¨å¤æ‚å¤šæ­¥éª¤ä»»åŠ¡ä¸­è¡¨ç°ä¼˜ç§€")
        elif overall_score >= 0.5:
            print("âš ï¸ è®°å¿†ç³»ç»Ÿè¡¨ç°è‰¯å¥½ï¼Œä½†å¤æ‚åœºæ™¯ä¸‹ä»æœ‰æ”¹è¿›ç©ºé—´")
        else:
            print("âŒ è®°å¿†ç³»ç»Ÿåœ¨å¤æ‚åœºæ™¯ä¸‹éœ€è¦æ˜¾è‘—æ”¹è¿›")
        
        return {
            "overall_score": overall_score,
            "total_memories": total_memories,
            "scenario1": scenario1_result,
            "scenario2": scenario2_result,
            "scenario3": scenario3_result,
            "performance_metrics": {
                "step_coherence": scenario1_result['avg_coherence'],
                "retrieval_accuracy": scenario1_result['avg_retrieval_accuracy'],
                "domain_integration": avg_integration,
                "search_relevance": avg_relevance,
                "cross_domain_capability": avg_cross_domain,
                "persistence_consistency": scenario3_result['retrieval_consistency'],
                "memory_quality": scenario3_result['persistence_quality']
            }
        }

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    tester = ComplexScenarioTester(Path("/root/code/claude-memory-test"))
    
    try:
        results = await tester.run_comprehensive_complex_test()
        
        # å­˜å‚¨æµ‹è¯•ç»“æœ
        results_file = Path("/root/code/claude-memory-test/complex_scenarios_test_results.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nğŸ“ è¯¦ç»†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        # å°†æµ‹è¯•ç»éªŒå­˜å‚¨åˆ°è®°å¿†ç³»ç»Ÿ
        tester.memory.remember(
            content=f"""
å¤æ‚å¤šæ­¥éª¤ä»»åŠ¡è®°å¿†ç®¡ç†æµ‹è¯•å®Œæˆæ€»ç»“ï¼š

æµ‹è¯•åœºæ™¯ï¼š
1. å¤šæ­¥éª¤Webåº”ç”¨å¼€å‘ - æµ‹è¯•æ­¥éª¤é—´è®°å¿†è¿è´¯æ€§å’Œè·¨æ­¥éª¤æ£€ç´¢
2. çŸ¥è¯†é¢†åŸŸæ•´åˆåº”ç”¨ - æµ‹è¯•è·¨é¢†åŸŸè®°å¿†æ•´åˆå’Œå…³è”èƒ½åŠ›  
3. é•¿æœŸè®°å¿†æŒä¹…åŒ– - æµ‹è¯•è®°å¿†è´¨é‡å’Œæ£€ç´¢ä¸€è‡´æ€§

å…³é”®å‘ç°ï¼š
- æ­¥éª¤è¿è´¯æ€§è¯„åˆ†: {results['performance_metrics']['step_coherence']:.3f}
- è·¨æ­¥éª¤æ£€ç´¢å‡†ç¡®æ€§: {results['performance_metrics']['retrieval_accuracy']:.3f}
- é¢†åŸŸæ•´åˆèƒ½åŠ›: {results['performance_metrics']['domain_integration']:.3f}
- æœç´¢ç›¸å…³æ€§: {results['performance_metrics']['search_relevance']:.3f}
- è·¨åŸŸæ•´åˆèƒ½åŠ›: {results['performance_metrics']['cross_domain_capability']:.3f}
- é•¿æœŸä¸€è‡´æ€§: {results['performance_metrics']['persistence_consistency']:.3f}

æ•´ä½“è¯„åˆ†: {results['overall_score']:.3f}/1.0

ç»“è®ºï¼šè®°å¿†ç³»ç»Ÿåœ¨å¤æ‚å¤šæ­¥éª¤ä»»åŠ¡ä¸­è¡¨ç°{'ä¼˜ç§€' if results['overall_score'] >= 0.7 else 'è‰¯å¥½' if results['overall_score'] >= 0.5 else 'éœ€è¦æ”¹è¿›'}ï¼Œ
èƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒå¤šæ­¥éª¤å·¥ä½œæµçš„è®°å¿†ç®¡ç†å’ŒçŸ¥è¯†æ•´åˆã€‚
            """,
            memory_type="episodic",
            title="å¤æ‚åœºæ™¯è®°å¿†ç®¡ç†æµ‹è¯•æ€»ç»“",
            tags=["æµ‹è¯•", "å¤æ‚åœºæ™¯", "å¤šæ­¥éª¤ä»»åŠ¡", "è®°å¿†ç®¡ç†", "è¯„ä¼°"],
            importance=9.0,
            scope="global"
        )
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())